

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.metrics.classification &mdash; BTB 0.3.2.dev0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../../../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#install">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-using-pip">Install using Pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-for-development">Install for Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#quickstart">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#tuners">Tuners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../readme.html#selectors">Selectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#what-s-next">Whatâ€™s next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#citing-btb">Citing BTB</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/btb.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.benchmark.html">btb.benchmark package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.benchmark.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.benchmark.html#module-btb.benchmark">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.selection.html">btb.selection package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#module-btb.selection">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.tuning.html">btb.tuning package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#module-btb.tuning">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#module-btb">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id1">0.3.1 - 2019-11-25</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#resolved-issues">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id2">0.3.0 - 2019-11-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-project-structure">New project structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-api">New API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id3">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id4">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id5">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id6">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id7">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id8">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id9">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id10">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id11">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id12">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id13">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id14">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id15">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id16">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id17">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id18">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id19">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id20">0.1.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sklearn.metrics.classification</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sklearn.metrics.classification</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Metrics to assess performance on classification task given class prediction</span>

<span class="sd">Functions named as ``*_score`` return a scalar value to maximize: the higher</span>
<span class="sd">the better</span>

<span class="sd">Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize:</span>
<span class="sd">the lower the better</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="c1">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="c1">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="c1">#          Arnaud Joly &lt;a.joly@ulg.ac.be&gt;</span>
<span class="c1">#          Jochen Wersdorfer &lt;jochen@wersdoerfer.de&gt;</span>
<span class="c1">#          Lars Buitinck</span>
<span class="c1">#          Joel Nothman &lt;joel.nothman@gmail.com&gt;</span>
<span class="c1">#          Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="c1">#          Jatin Shah &lt;jatindshah@gmail.com&gt;</span>
<span class="c1">#          Saurabh Jha &lt;saurabh.jhaa@gmail.com&gt;</span>
<span class="c1">#          Bernardo Stein &lt;bernardovstein@gmail.com&gt;</span>
<span class="c1">#          Shangwu Yao &lt;shangwuyao@gmail.com&gt;</span>
<span class="c1"># License: BSD 3 clause</span>


<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">coo_matrix</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csr_matrix</span>

<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">assert_all_finite</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_consistent_length</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">column_or_1d</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">unique_labels</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">type_of_target</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">_num_samples</span>
<span class="kn">from</span> <span class="nn">..utils.sparsefuncs</span> <span class="kn">import</span> <span class="n">count_nonzero</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">UndefinedMetricWarning</span>


<span class="k">def</span> <span class="nf">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Check that y_true and y_pred belong to the same classification task</span>

<span class="sd">    This converts multiclass or binary types to a common shape, and raises a</span>
<span class="sd">    ValueError for a mix of multilabel and multiclass targets, a mix of</span>
<span class="sd">    multilabel formats, for the presence of continuous-valued or multioutput</span>
<span class="sd">    targets, or for targets of different lengths.</span>

<span class="sd">    Column vectors are squeezed to 1d, while multilabel formats are returned</span>
<span class="sd">    as CSR sparse label indicators.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array-like</span>

<span class="sd">    y_pred : array-like</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    type_true : one of {&#39;multilabel-indicator&#39;, &#39;multiclass&#39;, &#39;binary&#39;}</span>
<span class="sd">        The type of the true target data, as output by</span>
<span class="sd">        ``utils.multiclass.type_of_target``</span>

<span class="sd">    y_true : array or indicator matrix</span>

<span class="sd">    y_pred : array or indicator matrix</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">type_true</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">type_pred</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="n">y_type</span> <span class="o">=</span> <span class="p">{</span><span class="n">type_true</span><span class="p">,</span> <span class="n">type_pred</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">y_type</span> <span class="o">==</span> <span class="p">{</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">}:</span>
        <span class="n">y_type</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;multiclass&quot;</span><span class="p">}</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_type</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Classification metrics can&#39;t handle a mix of </span><span class="si">{0}</span><span class="s2"> &quot;</span>
                         <span class="s2">&quot;and </span><span class="si">{1}</span><span class="s2"> targets&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">type_true</span><span class="p">,</span> <span class="n">type_pred</span><span class="p">))</span>

    <span class="c1"># We can&#39;t have more than one value on y_type =&gt; The set is no more needed</span>
    <span class="n">y_type</span> <span class="o">=</span> <span class="n">y_type</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>

    <span class="c1"># No metrics support &quot;multiclass-multioutput&quot; format</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">y_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">,</span> <span class="s2">&quot;multilabel-indicator&quot;</span><span class="p">]):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0}</span><span class="s2"> is not supported&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_type</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">y_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">]:</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y_type</span> <span class="o">==</span> <span class="s2">&quot;binary&quot;</span><span class="p">:</span>
            <span class="n">unique_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">union1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_values</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">y_type</span> <span class="o">=</span> <span class="s2">&quot;multiclass&quot;</span>

    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">y_type</span> <span class="o">=</span> <span class="s1">&#39;multilabel-indicator&#39;</span>

    <span class="k">return</span> <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span>


<span class="k">def</span> <span class="nf">_weighted_sum</span><span class="p">(</span><span class="n">sample_score</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">sample_score</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sample_score</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">sample_score</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Accuracy classification score.</span>

<span class="sd">    In multilabel classification, this function computes subset accuracy:</span>
<span class="sd">    the set of labels predicted for a sample must *exactly* match the</span>
<span class="sd">    corresponding set of labels in y_true.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;accuracy_score&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    normalize : bool, optional (default=True)</span>
<span class="sd">        If ``False``, return the number of correctly classified samples.</span>
<span class="sd">        Otherwise, return the fraction of correctly classified samples.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        If ``normalize == True``, return the fraction of correctly</span>
<span class="sd">        classified samples (float), else returns the number of correctly</span>
<span class="sd">        classified samples (int).</span>

<span class="sd">        The best performance is 1 with ``normalize == True`` and the number</span>
<span class="sd">        of samples with ``normalize == False``.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    jaccard_score, hamming_loss, zero_one_loss</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    In binary and multiclass classification, this function is equal</span>
<span class="sd">    to the ``jaccard_score`` function.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import accuracy_score</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 3]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 3]</span>
<span class="sd">    &gt;&gt;&gt; accuracy_score(y_true, y_pred)</span>
<span class="sd">    0.5</span>
<span class="sd">    &gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False)</span>
<span class="sd">    2</span>

<span class="sd">    In the multilabel case with binary label indicators:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))</span>
<span class="sd">    0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Compute accuracy for each possible representation</span>
    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="n">differing_labels</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">differing_labels</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span>

    <span class="k">return</span> <span class="n">_weighted_sum</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">normalize</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification</span>

<span class="sd">    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`</span>
<span class="sd">    is equal to the number of observations known to be in group :math:`i` but</span>
<span class="sd">    predicted to be in group :math:`j`.</span>

<span class="sd">    Thus in binary classification, the count of true negatives is</span>
<span class="sd">    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is</span>
<span class="sd">    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;confusion_matrix&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array, shape = [n_samples]</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : array, shape = [n_samples]</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : array, shape = [n_classes], optional</span>
<span class="sd">        List of labels to index the matrix. This may be used to reorder</span>
<span class="sd">        or select a subset of labels.</span>
<span class="sd">        If none is given, those that appear at least once</span>
<span class="sd">        in ``y_true`` or ``y_pred`` are used in sorted order.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    C : array, shape = [n_classes, n_classes]</span>
<span class="sd">        Confusion matrix</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Confusion matrix</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Confusion_matrix&gt;`_</span>
<span class="sd">           (Wikipedia and other references may use a different</span>
<span class="sd">           convention for axes)</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import confusion_matrix</span>
<span class="sd">    &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]</span>
<span class="sd">    &gt;&gt;&gt; confusion_matrix(y_true, y_pred)</span>
<span class="sd">    array([[2, 0, 0],</span>
<span class="sd">           [0, 0, 1],</span>
<span class="sd">           [1, 0, 2]])</span>

<span class="sd">    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span>
<span class="sd">    &gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span>
<span class="sd">    array([[2, 0, 0],</span>
<span class="sd">           [0, 0, 1],</span>
<span class="sd">           [1, 0, 2]])</span>

<span class="sd">    In the binary case, we can extract true positives, etc as follows:</span>

<span class="sd">    &gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()</span>
<span class="sd">    &gt;&gt;&gt; (tn, fp, fn, tp)</span>
<span class="sd">    (0, 2, 1, 1)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="n">y_type</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">([</span><span class="n">l</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">y_true</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;At least one label specified must be in y_true&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">n_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span>
    <span class="n">label_to_ind</span> <span class="o">=</span> <span class="p">{</span><span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">)}</span>
    <span class="c1"># convert yt, yp into index</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_to_ind</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_labels</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">])</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">label_to_ind</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_labels</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">y_true</span><span class="p">])</span>

    <span class="c1"># intersect y_pred, y_true with labels, eliminate items not in labels</span>
    <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="n">n_labels</span><span class="p">,</span> <span class="n">y_true</span> <span class="o">&lt;</span> <span class="n">n_labels</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="c1"># also eliminate weights of eliminated items</span>
    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>

    <span class="c1"># Choose the accumulator dtype to always have high precision</span>
    <span class="k">if</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">kind</span> <span class="ow">in</span> <span class="p">{</span><span class="s1">&#39;i&#39;</span><span class="p">,</span> <span class="s1">&#39;u&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">}:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>

    <span class="n">CM</span> <span class="o">=</span> <span class="n">coo_matrix</span><span class="p">((</span><span class="n">sample_weight</span><span class="p">,</span> <span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)),</span>
                    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_labels</span><span class="p">,</span> <span class="n">n_labels</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">CM</span>


<span class="k">def</span> <span class="nf">multilabel_confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">samplewise</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute a confusion matrix for each class or sample</span>

<span class="sd">    .. versionadded:: 0.21</span>

<span class="sd">    Compute class-wise (default) or sample-wise (samplewise=True) multilabel</span>
<span class="sd">    confusion matrix to evaluate the accuracy of a classification, and output</span>
<span class="sd">    confusion matrices for each class or sample.</span>

<span class="sd">    In multilabel confusion matrix :math:`MCM`, the count of true negatives</span>
<span class="sd">    is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,</span>
<span class="sd">    true positives is :math:`MCM_{:,1,1}` and false positives is</span>
<span class="sd">    :math:`MCM_{:,0,1}`.</span>

<span class="sd">    Multiclass data will be treated as if binarized under a one-vs-rest</span>
<span class="sd">    transformation. Returned confusion matrices will be in the order of</span>
<span class="sd">    sorted unique labels in the union of (y_true, y_pred).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;multilabel_confusion_matrix&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        of shape (n_samples, n_outputs) or (n_samples,)</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        of shape (n_samples, n_outputs) or (n_samples,)</span>
<span class="sd">        Estimated targets as returned by a classifier</span>

<span class="sd">    sample_weight : array-like of shape = (n_samples,), optional</span>
<span class="sd">        Sample weights</span>

<span class="sd">    labels : array-like</span>
<span class="sd">        A list of classes or column indices to select some (or to force</span>
<span class="sd">        inclusion of classes absent from the data)</span>

<span class="sd">    samplewise : bool, default=False</span>
<span class="sd">        In the multilabel case, this calculates a confusion matrix per sample</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    multi_confusion : array, shape (n_outputs, 2, 2)</span>
<span class="sd">        A 2x2 confusion matrix corresponding to each output in the input.</span>
<span class="sd">        When calculating class-wise multi_confusion (default), then</span>
<span class="sd">        n_outputs = n_labels; when calculating sample-wise multi_confusion</span>
<span class="sd">        (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,</span>
<span class="sd">        the results will be returned in the order specified in ``labels``,</span>
<span class="sd">        otherwise the results will be returned in sorted order by default.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    confusion_matrix</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The multilabel_confusion_matrix calculates class-wise or sample-wise</span>
<span class="sd">    multilabel confusion matrices, and in multiclass tasks, labels are</span>
<span class="sd">    binarized under a one-vs-rest way; while confusion_matrix calculates</span>
<span class="sd">    one confusion matrix for confusion between every two classes.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    Multilabel-indicator case:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import multilabel_confusion_matrix</span>
<span class="sd">    &gt;&gt;&gt; y_true = np.array([[1, 0, 1],</span>
<span class="sd">    ...                    [0, 1, 0]])</span>
<span class="sd">    &gt;&gt;&gt; y_pred = np.array([[1, 0, 0],</span>
<span class="sd">    ...                    [0, 1, 1]])</span>
<span class="sd">    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred)</span>
<span class="sd">    array([[[1, 0],</span>
<span class="sd">            [0, 1]],</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">           [[1, 0],</span>
<span class="sd">            [0, 1]],</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">           [[0, 1],</span>
<span class="sd">            [1, 0]]])</span>

<span class="sd">    Multiclass case:</span>

<span class="sd">    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]</span>
<span class="sd">    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred,</span>
<span class="sd">    ...                             labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])</span>
<span class="sd">    array([[[3, 1],</span>
<span class="sd">            [0, 2]],</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">           [[5, 0],</span>
<span class="sd">            [1, 0]],</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">           [[2, 1],</span>
<span class="sd">            [1, 2]]])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">,</span> <span class="s2">&quot;multilabel-indicator&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="n">y_type</span><span class="p">)</span>

    <span class="n">present_labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">present_labels</span>
        <span class="n">n_labels</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">labels</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">present_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                                 <span class="n">assume_unique</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>

    <span class="k">if</span> <span class="n">y_true</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">samplewise</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Samplewise metrics are not available outside of &quot;</span>
                             <span class="s2">&quot;multilabel classification.&quot;</span><span class="p">)</span>

        <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
        <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">sorted_labels</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">classes_</span>

        <span class="c1"># labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span>
        <span class="n">tp_bins</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">tp</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)[</span><span class="n">tp</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tp_bins_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">tp_bins</span><span class="p">):</span>
            <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">tp_bins</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">tp_bins_weights</span><span class="p">,</span>
                                 <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Pathological case</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">):</span>
            <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                   <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">):</span>
            <span class="n">true_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                   <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

        <span class="c1"># Retain only selected labels</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">sorted_labels</span><span class="p">,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_labels</span><span class="p">])</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">tp_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">true_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
        <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">pred_sum</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">sum_axis</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">samplewise</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># All labels are index integers for multilabel.</span>
        <span class="c1"># Select labels:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">present_labels</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">present_labels</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All labels must be in [0, n labels) for &#39;</span>
                                 <span class="s1">&#39;multilabel targets. &#39;</span>
                                 <span class="s1">&#39;Got </span><span class="si">%d</span><span class="s1"> &gt; </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">present_labels</span><span class="p">)))</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All labels must be in [0, n labels) for &#39;</span>
                                 <span class="s1">&#39;multilabel targets. &#39;</span>
                                 <span class="s1">&#39;Got </span><span class="si">%d</span><span class="s1"> &lt; 0&#39;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">n_labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[:,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_labels</span><span class="p">]]</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">labels</span><span class="p">[:</span><span class="n">n_labels</span><span class="p">]]</span>

        <span class="c1"># calculate weighted counts</span>
        <span class="n">true_and_pred</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">true_and_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">sum_axis</span><span class="p">,</span>
                               <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">sum_axis</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">sum_axis</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">fp</span> <span class="o">=</span> <span class="n">pred_sum</span> <span class="o">-</span> <span class="n">tp_sum</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">true_sum</span> <span class="o">-</span> <span class="n">tp_sum</span>
    <span class="n">tp</span> <span class="o">=</span> <span class="n">tp_sum</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">samplewise</span><span class="p">:</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">tp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tp</span><span class="p">)</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">tp</span> <span class="o">-</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">fn</span>
    <span class="k">elif</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span> <span class="o">-</span> <span class="n">tp</span> <span class="o">-</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">fn</span>
    <span class="k">elif</span> <span class="n">samplewise</span><span class="p">:</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">tp</span> <span class="o">-</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">fn</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tn</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">tp</span> <span class="o">-</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">fn</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cohen_kappa_score</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Cohen&#39;s kappa: a statistic that measures inter-annotator agreement.</span>

<span class="sd">    This function computes Cohen&#39;s kappa [1]_, a score that expresses the level</span>
<span class="sd">    of agreement between two annotators on a classification problem. It is</span>
<span class="sd">    defined as</span>

<span class="sd">    .. math::</span>
<span class="sd">        \kappa = (p_o - p_e) / (1 - p_e)</span>

<span class="sd">    where :math:`p_o` is the empirical probability of agreement on the label</span>
<span class="sd">    assigned to any sample (the observed agreement ratio), and :math:`p_e` is</span>
<span class="sd">    the expected agreement when both annotators assign labels randomly.</span>
<span class="sd">    :math:`p_e` is estimated using a per-annotator empirical prior over the</span>
<span class="sd">    class labels [2]_.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;cohen_kappa&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y1 : array, shape = [n_samples]</span>
<span class="sd">        Labels assigned by the first annotator.</span>

<span class="sd">    y2 : array, shape = [n_samples]</span>
<span class="sd">        Labels assigned by the second annotator. The kappa statistic is</span>
<span class="sd">        symmetric, so swapping ``y1`` and ``y2`` doesn&#39;t change the value.</span>

<span class="sd">    labels : array, shape = [n_classes], optional</span>
<span class="sd">        List of labels to index the matrix. This may be used to select a</span>
<span class="sd">        subset of labels. If None, all labels that appear at least once in</span>
<span class="sd">        ``y1`` or ``y2`` are used.</span>

<span class="sd">    weights : str, optional</span>
<span class="sd">        List of weighting type to calculate the score. None means no weighted;</span>
<span class="sd">        &quot;linear&quot; means linear weighted; &quot;quadratic&quot; means quadratic weighted.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    kappa : float</span>
<span class="sd">        The kappa statistic, which is a number between -1 and 1. The maximum</span>
<span class="sd">        value means complete agreement; zero or lower means chance agreement.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] J. Cohen (1960). &quot;A coefficient of agreement for nominal scales&quot;.</span>
<span class="sd">           Educational and Psychological Measurement 20(1):37-46.</span>
<span class="sd">           doi:10.1177/001316446002000104.</span>
<span class="sd">    .. [2] `R. Artstein and M. Poesio (2008). &quot;Inter-coder agreement for</span>
<span class="sd">           computational linguistics&quot;. Computational Linguistics 34(4):555-596.</span>
<span class="sd">           &lt;https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2&gt;`_</span>
<span class="sd">    .. [3] `Wikipedia entry for the Cohen&#39;s kappa.</span>
<span class="sd">            &lt;https://en.wikipedia.org/wiki/Cohen%27s_kappa&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">confusion</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">confusion</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">sum0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sum1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">sum0</span><span class="p">,</span> <span class="n">sum1</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sum0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="n">w_mat</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span> <span class="ow">or</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span>
        <span class="n">w_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="n">w_mat</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="n">w_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">w_mat</span> <span class="o">-</span> <span class="n">w_mat</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">w_mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">w_mat</span> <span class="o">-</span> <span class="n">w_mat</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown kappa weighting type.&quot;</span><span class="p">)</span>

    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w_mat</span> <span class="o">*</span> <span class="n">confusion</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w_mat</span> <span class="o">*</span> <span class="n">expected</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">k</span>


<span class="k">def</span> <span class="nf">jaccard_similarity_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                             <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Jaccard similarity coefficient score</span>

<span class="sd">    .. deprecated:: 0.21</span>
<span class="sd">        This is deprecated to be removed in 0.23, since its handling of</span>
<span class="sd">        binary and multiclass inputs was broken. `jaccard_score` has an API</span>
<span class="sd">        that is consistent with precision_score, f_score, etc.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;jaccard_similarity_score&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    normalize : bool, optional (default=True)</span>
<span class="sd">        If ``False``, return the sum of the Jaccard similarity coefficient</span>
<span class="sd">        over the sample set. Otherwise, return the average of Jaccard</span>
<span class="sd">        similarity coefficient.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        If ``normalize == True``, return the average Jaccard similarity</span>
<span class="sd">        coefficient, else it returns the sum of the Jaccard similarity</span>
<span class="sd">        coefficient over the sample set.</span>

<span class="sd">        The best performance is 1 with ``normalize == True`` and the number</span>
<span class="sd">        of samples with ``normalize == False``.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    accuracy_score, hamming_loss, zero_one_loss</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    In binary and multiclass classification, this function is equivalent</span>
<span class="sd">    to the ``accuracy_score``. It differs in the multilabel classification</span>
<span class="sd">    problem.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Jaccard index</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Jaccard_index&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;jaccard_similarity_score has been deprecated and replaced &#39;</span>
                  <span class="s1">&#39;with jaccard_score. It will be removed in version 0.23. &#39;</span>
                  <span class="s1">&#39;This implementation has surprising behavior for binary &#39;</span>
                  <span class="s1">&#39;and multiclass classification tasks.&#39;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>

    <span class="c1"># Compute accuracy for each possible representation</span>
    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
            <span class="c1"># oddly, we may get an &quot;invalid&quot; rather than a &quot;divide&quot; error here</span>
            <span class="n">pred_or_true</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_true</span> <span class="o">+</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pred_and_true</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">score</span> <span class="o">=</span> <span class="n">pred_and_true</span> <span class="o">/</span> <span class="n">pred_or_true</span>
            <span class="n">score</span><span class="p">[</span><span class="n">pred_or_true</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">==</span> <span class="n">y_pred</span>

    <span class="k">return</span> <span class="n">_weighted_sum</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">normalize</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Jaccard similarity coefficient score</span>

<span class="sd">    The Jaccard index [1], or Jaccard similarity coefficient, defined as</span>
<span class="sd">    the size of the intersection divided by the size of the union of two label</span>
<span class="sd">    sets, is used to compare set of predicted labels for a sample to the</span>
<span class="sd">    corresponding set of labels in ``y_true``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;jaccard_score&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None, &#39;binary&#39; (default), &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average, weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float (if average is not None) or array of floats, shape =\</span>
<span class="sd">            [n_unique_labels]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    accuracy_score, f_score, multilabel_confusion_matrix</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    :func:`jaccard_score` may be a poor metric if there are no</span>
<span class="sd">    positives for some samples or classes. Jaccard is undefined if there are</span>
<span class="sd">    no true or predicted labels, and our implementation will return a score</span>
<span class="sd">    of 0 with a warning.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Jaccard index</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Jaccard_index&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import jaccard_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = np.array([[0, 1, 1],</span>
<span class="sd">    ...                    [1, 1, 0]])</span>
<span class="sd">    &gt;&gt;&gt; y_pred = np.array([[1, 1, 1],</span>
<span class="sd">    ...                    [1, 0, 0]])</span>

<span class="sd">    In the binary case:</span>

<span class="sd">    &gt;&gt;&gt; jaccard_score(y_true[0], y_pred[0])  # doctest: +ELLIPSIS</span>
<span class="sd">    0.6666...</span>

<span class="sd">    In the multilabel case:</span>

<span class="sd">    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=&#39;samples&#39;)</span>
<span class="sd">    0.5833...</span>
<span class="sd">    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    0.6666...</span>
<span class="sd">    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([0.5, 0.5, 1. ])</span>

<span class="sd">    In the multiclass case:</span>

<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 2]</span>
<span class="sd">    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=None)</span>
<span class="sd">    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS</span>
<span class="sd">    array([1. , 0. , 0.33...])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">_check_set_wise_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                    <span class="n">pos_label</span><span class="p">)</span>
    <span class="n">samplewise</span> <span class="o">=</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span>
    <span class="n">MCM</span> <span class="o">=</span> <span class="n">multilabel_confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                      <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">samplewise</span><span class="o">=</span><span class="n">samplewise</span><span class="p">)</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;micro&#39;</span><span class="p">:</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">numerator</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">denominator</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>

    <span class="n">jaccard</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">denominator</span><span class="p">,</span> <span class="s1">&#39;jaccard&#39;</span><span class="p">,</span>
                          <span class="s1">&#39;true or predicted&#39;</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="p">(</span><span class="s1">&#39;jaccard&#39;</span><span class="p">,))</span>
    <span class="k">if</span> <span class="n">average</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">jaccard</span>
    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;weighted&#39;</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
            <span class="c1"># numerator is 0, and warning should have already been issued</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span> <span class="ow">and</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">sample_weight</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">jaccard</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">matthews_corrcoef</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the Matthews correlation coefficient (MCC)</span>

<span class="sd">    The Matthews correlation coefficient is used in machine learning as a</span>
<span class="sd">    measure of the quality of binary and multiclass classifications. It takes</span>
<span class="sd">    into account true and false positives and negatives and is generally</span>
<span class="sd">    regarded as a balanced measure which can be used even if the classes are of</span>
<span class="sd">    very different sizes. The MCC is in essence a correlation coefficient value</span>
<span class="sd">    between -1 and +1. A coefficient of +1 represents a perfect prediction, 0</span>
<span class="sd">    an average random prediction and -1 an inverse prediction.  The statistic</span>
<span class="sd">    is also known as the phi coefficient. [source: Wikipedia]</span>

<span class="sd">    Binary and multiclass labels are supported.  Only in the binary case does</span>
<span class="sd">    this relate to information about true and false positives and negatives.</span>
<span class="sd">    See references below.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;matthews_corrcoef&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array, shape = [n_samples]</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : array, shape = [n_samples]</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], default None</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    mcc : float</span>
<span class="sd">        The Matthews correlation coefficient (+1 represents a perfect</span>
<span class="sd">        prediction, 0 an average random prediction and -1 and inverse</span>
<span class="sd">        prediction).</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the</span>
<span class="sd">       accuracy of prediction algorithms for classification: an overview</span>
<span class="sd">       &lt;https://doi.org/10.1093/bioinformatics/16.5.412&gt;`_</span>

<span class="sd">    .. [2] `Wikipedia entry for the Matthews Correlation Coefficient</span>
<span class="sd">       &lt;https://en.wikipedia.org/wiki/Matthews_correlation_coefficient&gt;`_</span>

<span class="sd">    .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a</span>
<span class="sd">        K-category correlation coefficient</span>
<span class="sd">        &lt;https://www.sciencedirect.com/science/article/pii/S1476927104000799&gt;`_</span>

<span class="sd">    .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN</span>
<span class="sd">        Error Measures in MultiClass Prediction</span>
<span class="sd">        &lt;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef</span>
<span class="sd">    &gt;&gt;&gt; y_true = [+1, +1, +1, -1]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [+1, -1, +1, +1]</span>
<span class="sd">    &gt;&gt;&gt; matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS</span>
<span class="sd">    -0.33...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">}:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> is not supported&quot;</span> <span class="o">%</span> <span class="n">y_type</span><span class="p">)</span>

    <span class="n">lb</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
    <span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">]))</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">t_sum</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">p_sum</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">n_correct</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">p_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">cov_ytyp</span> <span class="o">=</span> <span class="n">n_correct</span> <span class="o">*</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t_sum</span><span class="p">,</span> <span class="n">p_sum</span><span class="p">)</span>
    <span class="n">cov_ypyp</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p_sum</span><span class="p">,</span> <span class="n">p_sum</span><span class="p">)</span>
    <span class="n">cov_ytyt</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t_sum</span><span class="p">,</span> <span class="n">t_sum</span><span class="p">)</span>
    <span class="n">mcc</span> <span class="o">=</span> <span class="n">cov_ytyp</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cov_ytyt</span> <span class="o">*</span> <span class="n">cov_ypyp</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">mcc</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mcc</span>


<span class="k">def</span> <span class="nf">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Zero-one classification loss.</span>

<span class="sd">    If normalize is ``True``, return the fraction of misclassifications</span>
<span class="sd">    (float), else it returns the number of misclassifications (int). The best</span>
<span class="sd">    performance is 0.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;zero_one_loss&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    normalize : bool, optional (default=True)</span>
<span class="sd">        If ``False``, return the number of misclassifications.</span>
<span class="sd">        Otherwise, return the fraction of misclassifications.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float or int,</span>
<span class="sd">        If ``normalize == True``, return the fraction of misclassifications</span>
<span class="sd">        (float), else it returns the number of misclassifications (int).</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    In multilabel classification, the zero_one_loss function corresponds to</span>
<span class="sd">    the subset zero-one loss: for each sample, the entire set of labels must be</span>
<span class="sd">    correctly predicted, otherwise the loss for that sample is equal to one.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    accuracy_score, hamming_loss, jaccard_score</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import zero_one_loss</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [1, 2, 3, 4]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [2, 2, 3, 4]</span>
<span class="sd">    &gt;&gt;&gt; zero_one_loss(y_true, y_pred)</span>
<span class="sd">    0.25</span>
<span class="sd">    &gt;&gt;&gt; zero_one_loss(y_true, y_pred, normalize=False)</span>
<span class="sd">    1</span>

<span class="sd">    In the multilabel case with binary label indicators:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))</span>
<span class="sd">    0.5</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                           <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
                           <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">score</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">_num_samples</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
             <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the F1 score, also known as balanced F-score or F-measure</span>

<span class="sd">    The F1 score can be interpreted as a weighted average of the precision and</span>
<span class="sd">    recall, where an F1 score reaches its best value at 1 and worst score at 0.</span>
<span class="sd">    The relative contribution of precision and recall to the F1 score are</span>
<span class="sd">    equal. The formula for the F1 score is::</span>

<span class="sd">        F1 = 2 * (precision * recall) / (precision + recall)</span>

<span class="sd">    In the multi-class and multi-label case, this is the average of</span>
<span class="sd">    the F1 score of each class with weighting depending on the ``average``</span>
<span class="sd">    parameter.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           parameter *labels* improved for multiclass problem.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None, &#39;binary&#39; (default), &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        This parameter is required for multiclass/multilabel targets.</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    f1_score : float or array of float, shape = [n_unique_labels]</span>
<span class="sd">        F1 score of the positive class in binary classification or weighted</span>
<span class="sd">        average of the F1 scores of each class for the multiclass task.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    fbeta_score, precision_recall_fscore_support, jaccard_score,</span>
<span class="sd">    multilabel_confusion_matrix</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the F1-score</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import f1_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; f1_score(y_true, y_pred, average=&#39;macro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.26...</span>
<span class="sd">    &gt;&gt;&gt; f1_score(y_true, y_pred, average=&#39;micro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; f1_score(y_true, y_pred, average=&#39;weighted&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.26...</span>
<span class="sd">    &gt;&gt;&gt; f1_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([0.8, 0. , 0. ])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    When ``true positive + false positive == 0`` or</span>
<span class="sd">    ``true positive + false negative == 0``, f-score returns 0 and raises</span>
<span class="sd">    ``UndefinedMetricWarning``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                       <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                       <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the F-beta score</span>

<span class="sd">    The F-beta score is the weighted harmonic mean of precision and recall,</span>
<span class="sd">    reaching its optimal value at 1 and its worst value at 0.</span>

<span class="sd">    The `beta` parameter determines the weight of recall in the combined</span>
<span class="sd">    score. ``beta &lt; 1`` lends more weight to precision, while ``beta &gt; 1``</span>
<span class="sd">    favors recall (``beta -&gt; 0`` considers only precision, ``beta -&gt; inf``</span>
<span class="sd">    only recall).</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    beta : float</span>
<span class="sd">        Weight of precision in harmonic mean.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           parameter *labels* improved for multiclass problem.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None, &#39;binary&#39; (default), &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        This parameter is required for multiclass/multilabel targets.</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fbeta_score : float (if average is not None) or array of float, shape =\</span>
<span class="sd">        [n_unique_labels]</span>
<span class="sd">        F-beta score of the positive class in binary classification or weighted</span>
<span class="sd">        average of the F-beta score of each class for the multiclass task.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    precision_recall_fscore_support, multilabel_confusion_matrix</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).</span>
<span class="sd">           Modern Information Retrieval. Addison Wesley, pp. 327-328.</span>

<span class="sd">    .. [2] `Wikipedia entry for the F1-score</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import fbeta_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average=&#39;macro&#39;, beta=0.5)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    0.23...</span>
<span class="sd">    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average=&#39;micro&#39;, beta=0.5)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average=&#39;weighted&#39;, beta=0.5)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    0.23...</span>
<span class="sd">    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average=None, beta=0.5)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    array([0.71..., 0.        , 0.        ])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    When ``true positive + false positive == 0`` or</span>
<span class="sd">    ``true positive + false negative == 0``, f-score returns 0 and raises</span>
<span class="sd">    ``UndefinedMetricWarning``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                                 <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
                                                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                                 <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
                                                 <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                                                 <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;f-score&#39;</span><span class="p">,),</span>
                                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">_prf_divide</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">denominator</span><span class="p">,</span> <span class="n">metric</span><span class="p">,</span> <span class="n">modifier</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">warn_for</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Performs division and handles divide-by-zero.</span>

<span class="sd">    On zero-division, sets the corresponding result elements to zero</span>
<span class="sd">    and raises a warning.</span>

<span class="sd">    The metric, modifier and average arguments are used only for determining</span>
<span class="sd">    an appropriate warning.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">denominator</span> <span class="o">==</span> <span class="mf">0.0</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">denominator</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">denominator</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># avoid infs/nans</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="c1"># build appropriate warning</span>
    <span class="c1"># E.g. &quot;Precision and F-score are ill-defined and being set to 0.0 in</span>
    <span class="c1"># labels with no predicted samples&quot;</span>
    <span class="n">axis0</span> <span class="o">=</span> <span class="s1">&#39;sample&#39;</span>
    <span class="n">axis1</span> <span class="o">=</span> <span class="s1">&#39;label&#39;</span>
    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span><span class="p">:</span>
        <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span> <span class="o">=</span> <span class="n">axis1</span><span class="p">,</span> <span class="n">axis0</span>

    <span class="k">if</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">warn_for</span> <span class="ow">and</span> <span class="s1">&#39;f-score&#39;</span> <span class="ow">in</span> <span class="n">warn_for</span><span class="p">:</span>
        <span class="n">msg_start</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> and F-score are&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">title</span><span class="p">())</span>
    <span class="k">elif</span> <span class="n">metric</span> <span class="ow">in</span> <span class="n">warn_for</span><span class="p">:</span>
        <span class="n">msg_start</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> is&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">metric</span><span class="o">.</span><span class="n">title</span><span class="p">())</span>
    <span class="k">elif</span> <span class="s1">&#39;f-score&#39;</span> <span class="ow">in</span> <span class="n">warn_for</span><span class="p">:</span>
        <span class="n">msg_start</span> <span class="o">=</span> <span class="s1">&#39;F-score is&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> ill-defined and being set to 0.0 {{0}} &#39;</span>
           <span class="s1">&#39;no </span><span class="si">{1}</span><span class="s1"> </span><span class="si">{2}</span><span class="s1">s.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">msg_start</span><span class="p">,</span> <span class="n">modifier</span><span class="p">,</span> <span class="n">axis0</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;due to&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;in </span><span class="si">{0}</span><span class="s1">s with&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">axis1</span><span class="p">))</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">UndefinedMetricWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>


<span class="k">def</span> <span class="nf">_check_set_wise_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">pos_label</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Validation associated with set-wise metrics</span>

<span class="sd">    Returns identified labels</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">average_options</span> <span class="o">=</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="s1">&#39;samples&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">average</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">average_options</span> <span class="ow">and</span> <span class="n">average</span> <span class="o">!=</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;average has to be one of &#39;</span> <span class="o">+</span>
                         <span class="nb">str</span><span class="p">(</span><span class="n">average_options</span><span class="p">))</span>

    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">present_labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">y_type</span> <span class="o">==</span> <span class="s1">&#39;binary&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">pos_label</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">present_labels</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">present_labels</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;pos_label=</span><span class="si">%r</span><span class="s2"> is not a valid label: &quot;</span>
                                     <span class="s2">&quot;</span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">pos_label</span><span class="p">,</span> <span class="n">present_labels</span><span class="p">))</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos_label</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">average_options</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">average_options</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y_type</span> <span class="o">==</span> <span class="s1">&#39;multiclass&#39;</span><span class="p">:</span>
                <span class="n">average_options</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target is </span><span class="si">%s</span><span class="s2"> but average=&#39;binary&#39;. Please &quot;</span>
                             <span class="s2">&quot;choose another average setting, one of </span><span class="si">%r</span><span class="s2">.&quot;</span>
                             <span class="o">%</span> <span class="p">(</span><span class="n">y_type</span><span class="p">,</span> <span class="n">average_options</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">pos_label</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Note that pos_label (set to </span><span class="si">%r</span><span class="s2">) is ignored when &quot;</span>
                      <span class="s2">&quot;average != &#39;binary&#39; (got </span><span class="si">%r</span><span class="s2">). You may use &quot;</span>
                      <span class="s2">&quot;labels=[pos_label] to specify a single positive class.&quot;</span>
                      <span class="o">%</span> <span class="p">(</span><span class="n">pos_label</span><span class="p">,</span> <span class="n">average</span><span class="p">),</span> <span class="ne">UserWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">labels</span>


<span class="k">def</span> <span class="nf">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                    <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">,</span>
                                              <span class="s1">&#39;f-score&#39;</span><span class="p">),</span>
                                    <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute precision, recall, F-measure and support for each class</span>

<span class="sd">    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of</span>
<span class="sd">    true positives and ``fp`` the number of false positives. The precision is</span>
<span class="sd">    intuitively the ability of the classifier not to label as positive a sample</span>
<span class="sd">    that is negative.</span>

<span class="sd">    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of</span>
<span class="sd">    true positives and ``fn`` the number of false negatives. The recall is</span>
<span class="sd">    intuitively the ability of the classifier to find all the positive samples.</span>

<span class="sd">    The F-beta score can be interpreted as a weighted harmonic mean of</span>
<span class="sd">    the precision and recall, where an F-beta score reaches its best</span>
<span class="sd">    value at 1 and worst score at 0.</span>

<span class="sd">    The F-beta score weights recall more than precision by a factor of</span>
<span class="sd">    ``beta``. ``beta == 1.0`` means recall and precision are equally important.</span>

<span class="sd">    The support is the number of occurrences of each class in ``y_true``.</span>

<span class="sd">    If ``pos_label is None`` and in binary classification, this function</span>
<span class="sd">    returns the average precision, recall and F-measure if ``average``</span>
<span class="sd">    is one of ``&#39;micro&#39;``, ``&#39;macro&#39;``, ``&#39;weighted&#39;`` or ``&#39;samples&#39;``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    beta : float, 1.0 by default</span>
<span class="sd">        The strength of recall versus precision in the F-score.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None (default), &#39;binary&#39;, &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    warn_for : tuple or set, for internal use</span>
<span class="sd">        This determines which warnings will be made in the case that this</span>
<span class="sd">        function is being used to return only one of its metrics.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    precision : float (if average is not None) or array of float, shape =\</span>
<span class="sd">        [n_unique_labels]</span>

<span class="sd">    recall : float (if average is not None) or array of float, , shape =\</span>
<span class="sd">        [n_unique_labels]</span>

<span class="sd">    fbeta_score : float (if average is not None) or array of float, shape =\</span>
<span class="sd">        [n_unique_labels]</span>

<span class="sd">    support : int (if average is not None) or array of int, shape =\</span>
<span class="sd">        [n_unique_labels]</span>
<span class="sd">        The number of occurrences of each label in ``y_true``.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Precision and recall</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Precision_and_recall&gt;`_</span>

<span class="sd">    .. [2] `Wikipedia entry for the F1-score</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_</span>

<span class="sd">    .. [3] `Discriminative Methods for Multi-labeled Classification Advances</span>
<span class="sd">           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu</span>
<span class="sd">           Godbole, Sunita Sarawagi</span>
<span class="sd">           &lt;http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support</span>
<span class="sd">    &gt;&gt;&gt; y_true = np.array([&#39;cat&#39;, &#39;dog&#39;, &#39;pig&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;pig&#39;])</span>
<span class="sd">    &gt;&gt;&gt; y_pred = np.array([&#39;cat&#39;, &#39;pig&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;])</span>
<span class="sd">    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=&#39;macro&#39;)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    (0.22..., 0.33..., 0.26..., None)</span>
<span class="sd">    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=&#39;micro&#39;)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    (0.33..., 0.33..., 0.33..., None)</span>
<span class="sd">    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    (0.22..., 0.33..., 0.26..., None)</span>

<span class="sd">    It is possible to compute per-label precisions, recalls, F1-scores and</span>
<span class="sd">    supports instead of averaging:</span>

<span class="sd">    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None,</span>
<span class="sd">    ... labels=[&#39;pig&#39;, &#39;dog&#39;, &#39;cat&#39;])</span>
<span class="sd">    ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE</span>
<span class="sd">    (array([0.        , 0.        , 0.66...]),</span>
<span class="sd">     array([0., 0., 1.]), array([0. , 0. , 0.8]),</span>
<span class="sd">     array([2, 2, 2]))</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    When ``true positive + false positive == 0``, precision is undefined;</span>
<span class="sd">    When ``true positive + false negative == 0``, recall is undefined.</span>
<span class="sd">    In such cases, the metric will be set to 0, as will f-score, and</span>
<span class="sd">    ``UndefinedMetricWarning`` will be raised.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">beta</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;beta should be &gt;0 in the F-beta score&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">_check_set_wise_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
                                    <span class="n">pos_label</span><span class="p">)</span>

    <span class="c1"># Calculate tp_sum, pred_sum, true_sum ###</span>
    <span class="n">samplewise</span> <span class="o">=</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span>
    <span class="n">MCM</span> <span class="o">=</span> <span class="n">multilabel_confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                      <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">samplewise</span><span class="o">=</span><span class="n">samplewise</span><span class="p">)</span>
    <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">tp_sum</span> <span class="o">+</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">true_sum</span> <span class="o">=</span> <span class="n">tp_sum</span> <span class="o">+</span> <span class="n">MCM</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;micro&#39;</span><span class="p">:</span>
        <span class="n">tp_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tp_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">pred_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">pred_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">true_sum</span><span class="o">.</span><span class="n">sum</span><span class="p">()])</span>

    <span class="c1"># Finally, we have all our sufficient statistics. Divide! #</span>
    <span class="n">beta2</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="c1"># Divide, and on zero-division, set scores to 0 and warn:</span>

    <span class="n">precision</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">tp_sum</span><span class="p">,</span> <span class="n">pred_sum</span><span class="p">,</span>
                            <span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;predicted&#39;</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">warn_for</span><span class="p">)</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">_prf_divide</span><span class="p">(</span><span class="n">tp_sum</span><span class="p">,</span> <span class="n">true_sum</span><span class="p">,</span>
                         <span class="s1">&#39;recall&#39;</span><span class="p">,</span> <span class="s1">&#39;true&#39;</span><span class="p">,</span> <span class="n">average</span><span class="p">,</span> <span class="n">warn_for</span><span class="p">)</span>
    <span class="c1"># Don&#39;t need to warn for F: either P or R warned, or tp == 0 where pos</span>
    <span class="c1"># and true are nonzero, in which case, F is well-defined and zero</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span>
    <span class="n">denom</span><span class="p">[</span><span class="n">denom</span> <span class="o">==</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># avoid division by 0</span>
    <span class="n">f_score</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span> <span class="o">/</span> <span class="n">denom</span>

    <span class="c1"># Average the results</span>
    <span class="k">if</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;weighted&#39;</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">true_sum</span>
        <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">average</span> <span class="o">==</span> <span class="s1">&#39;samples&#39;</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">sample_weight</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">average</span> <span class="o">!=</span> <span class="s1">&#39;binary&#39;</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">f_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">f_score</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">true_sum</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># return no support</span>

    <span class="k">return</span> <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">f_score</span><span class="p">,</span> <span class="n">true_sum</span>


<span class="k">def</span> <span class="nf">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the precision</span>

<span class="sd">    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of</span>
<span class="sd">    true positives and ``fp`` the number of false positives. The precision is</span>
<span class="sd">    intuitively the ability of the classifier not to label as positive a sample</span>
<span class="sd">    that is negative.</span>

<span class="sd">    The best value is 1 and the worst value is 0.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           parameter *labels* improved for multiclass problem.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None, &#39;binary&#39; (default), &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        This parameter is required for multiclass/multilabel targets.</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    precision : float (if average is not None) or array of float, shape =\</span>
<span class="sd">        [n_unique_labels]</span>
<span class="sd">        Precision of the positive class in binary classification or weighted</span>
<span class="sd">        average of the precision of each class for the multiclass task.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    precision_recall_fscore_support, multilabel_confusion_matrix</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import precision_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; precision_score(y_true, y_pred, average=&#39;macro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.22...</span>
<span class="sd">    &gt;&gt;&gt; precision_score(y_true, y_pred, average=&#39;micro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; precision_score(y_true, y_pred, average=&#39;weighted&#39;)</span>
<span class="sd">    ... # doctest: +ELLIPSIS</span>
<span class="sd">    0.22...</span>
<span class="sd">    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS</span>
<span class="sd">    array([0.66..., 0.        , 0.        ])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    When ``true positive + false positive == 0``, precision returns 0 and</span>
<span class="sd">    raises ``UndefinedMetricWarning``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                                 <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
                                                 <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                                                 <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,),</span>
                                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span>
                 <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the recall</span>

<span class="sd">    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of</span>
<span class="sd">    true positives and ``fn`` the number of false negatives. The recall is</span>
<span class="sd">    intuitively the ability of the classifier to find all the positive samples.</span>

<span class="sd">    The best value is 1 and the worst value is 0.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : list, optional</span>
<span class="sd">        The set of labels to include when ``average != &#39;binary&#39;``, and their</span>
<span class="sd">        order if ``average is None``. Labels present in the data can be</span>
<span class="sd">        excluded, for example to calculate a multiclass average ignoring a</span>
<span class="sd">        majority negative class, while labels not present in the data will</span>
<span class="sd">        result in 0 components in a macro average. For multilabel targets,</span>
<span class="sd">        labels are column indices. By default, all labels in ``y_true`` and</span>
<span class="sd">        ``y_pred`` are used in sorted order.</span>

<span class="sd">        .. versionchanged:: 0.17</span>
<span class="sd">           parameter *labels* improved for multiclass problem.</span>

<span class="sd">    pos_label : str or int, 1 by default</span>
<span class="sd">        The class to report if ``average=&#39;binary&#39;`` and the data is binary.</span>
<span class="sd">        If the data are multiclass or multilabel, this will be ignored;</span>
<span class="sd">        setting ``labels=[pos_label]`` and ``average != &#39;binary&#39;`` will report</span>
<span class="sd">        scores for that label only.</span>

<span class="sd">    average : string, [None, &#39;binary&#39; (default), &#39;micro&#39;, &#39;macro&#39;, &#39;samples&#39;, \</span>
<span class="sd">                       &#39;weighted&#39;]</span>
<span class="sd">        This parameter is required for multiclass/multilabel targets.</span>
<span class="sd">        If ``None``, the scores for each class are returned. Otherwise, this</span>
<span class="sd">        determines the type of averaging performed on the data:</span>

<span class="sd">        ``&#39;binary&#39;``:</span>
<span class="sd">            Only report results for the class specified by ``pos_label``.</span>
<span class="sd">            This is applicable only if targets (``y_{true,pred}``) are binary.</span>
<span class="sd">        ``&#39;micro&#39;``:</span>
<span class="sd">            Calculate metrics globally by counting the total true positives,</span>
<span class="sd">            false negatives and false positives.</span>
<span class="sd">        ``&#39;macro&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their unweighted</span>
<span class="sd">            mean.  This does not take label imbalance into account.</span>
<span class="sd">        ``&#39;weighted&#39;``:</span>
<span class="sd">            Calculate metrics for each label, and find their average weighted</span>
<span class="sd">            by support (the number of true instances for each label). This</span>
<span class="sd">            alters &#39;macro&#39; to account for label imbalance; it can result in an</span>
<span class="sd">            F-score that is not between precision and recall.</span>
<span class="sd">        ``&#39;samples&#39;``:</span>
<span class="sd">            Calculate metrics for each instance, and find their average (only</span>
<span class="sd">            meaningful for multilabel classification where this differs from</span>
<span class="sd">            :func:`accuracy_score`).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    recall : float (if average is not None) or array of float, shape =\</span>
<span class="sd">        [n_unique_labels]</span>
<span class="sd">        Recall of the positive class in binary classification or weighted</span>
<span class="sd">        average of the recall of each class for the multiclass task.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    precision_recall_fscore_support, balanced_accuracy_score,</span>
<span class="sd">    multilabel_confusion_matrix</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import recall_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; recall_score(y_true, y_pred, average=&#39;macro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; recall_score(y_true, y_pred, average=&#39;micro&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; recall_score(y_true, y_pred, average=&#39;weighted&#39;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.33...</span>
<span class="sd">    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None)</span>
<span class="sd">    array([1., 0., 0.])</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    When ``true positive + false negative == 0``, recall returns 0 and raises</span>
<span class="sd">    ``UndefinedMetricWarning``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                                 <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                                 <span class="n">pos_label</span><span class="o">=</span><span class="n">pos_label</span><span class="p">,</span>
                                                 <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span>
                                                 <span class="n">warn_for</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;recall&#39;</span><span class="p">,),</span>
                                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span>


<span class="k">def</span> <span class="nf">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">adjusted</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the balanced accuracy</span>

<span class="sd">    The balanced accuracy in binary and multiclass classification problems to</span>
<span class="sd">    deal with imbalanced datasets. It is defined as the average of recall</span>
<span class="sd">    obtained on each class.</span>

<span class="sd">    The best value is 1 and the worst value is 0 when ``adjusted=False``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;balanced_accuracy_score&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    adjusted : bool, default=False</span>
<span class="sd">        When true, the result is adjusted for chance, so that random</span>
<span class="sd">        performance would score 0, and perfect performance scores 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    balanced_accuracy : float</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    recall_score, roc_auc_score</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Some literature promotes alternative definitions of balanced accuracy. Our</span>
<span class="sd">    definition is equivalent to :func:`accuracy_score` with class-balanced</span>
<span class="sd">    sample weights, and shares desirable properties with the binary case.</span>
<span class="sd">    See the :ref:`User Guide &lt;balanced_accuracy_score&gt;`.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).</span>
<span class="sd">           The balanced accuracy and its posterior distribution.</span>
<span class="sd">           Proceedings of the 20th International Conference on Pattern</span>
<span class="sd">           Recognition, 3121-24.</span>
<span class="sd">    .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D&#39;Arcy, (2015).</span>
<span class="sd">           `Fundamentals of Machine Learning for Predictive Data Analytics:</span>
<span class="sd">           Algorithms, Worked Examples, and Case Studies</span>
<span class="sd">           &lt;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&gt;`_.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import balanced_accuracy_score</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 0, 0, 1, 0]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 1, 0, 0, 0, 1]</span>
<span class="sd">    &gt;&gt;&gt; balanced_accuracy_score(y_true, y_pred)</span>
<span class="sd">    0.625</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
        <span class="n">per_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C</span><span class="p">)</span> <span class="o">/</span> <span class="n">C</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">per_class</span><span class="p">)):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;y_pred contains classes not in y_true&#39;</span><span class="p">)</span>
        <span class="n">per_class</span> <span class="o">=</span> <span class="n">per_class</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">per_class</span><span class="p">)]</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">per_class</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">adjusted</span><span class="p">:</span>
        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">per_class</span><span class="p">)</span>
        <span class="n">chance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_classes</span>
        <span class="n">score</span> <span class="o">-=</span> <span class="n">chance</span>
        <span class="n">score</span> <span class="o">/=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">chance</span>
    <span class="k">return</span> <span class="n">score</span>


<span class="k">def</span> <span class="nf">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">output_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a text report showing the main classification metrics</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;classification_report&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) target values.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Estimated targets as returned by a classifier.</span>

<span class="sd">    labels : array, shape = [n_labels]</span>
<span class="sd">        Optional list of label indices to include in the report.</span>

<span class="sd">    target_names : list of strings</span>
<span class="sd">        Optional display names matching the labels (same order).</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    digits : int</span>
<span class="sd">        Number of digits for formatting output floating point values.</span>
<span class="sd">        When ``output_dict`` is ``True``, this will be ignored and the</span>
<span class="sd">        returned values will not be rounded.</span>

<span class="sd">    output_dict : bool (default = False)</span>
<span class="sd">        If True, return output as dict</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    report : string / dict</span>
<span class="sd">        Text summary of the precision, recall, F1 score for each class.</span>
<span class="sd">        Dictionary returned if output_dict is True. Dictionary has the</span>
<span class="sd">        following structure::</span>

<span class="sd">            {&#39;label 1&#39;: {&#39;precision&#39;:0.5,</span>
<span class="sd">                         &#39;recall&#39;:1.0,</span>
<span class="sd">                         &#39;f1-score&#39;:0.67,</span>
<span class="sd">                         &#39;support&#39;:1},</span>
<span class="sd">             &#39;label 2&#39;: { ... },</span>
<span class="sd">              ...</span>
<span class="sd">            }</span>

<span class="sd">        The reported averages include macro average (averaging the unweighted</span>
<span class="sd">        mean per label), weighted average (averaging the support-weighted mean</span>
<span class="sd">        per label), sample average (only for multilabel classification) and</span>
<span class="sd">        micro average (averaging the total true positives, false negatives and</span>
<span class="sd">        false positives) it is only shown for multi-label or multi-class</span>
<span class="sd">        with a subset of classes because it is accuracy otherwise.</span>
<span class="sd">        See also:func:`precision_recall_fscore_support` for more details</span>
<span class="sd">        on averages.</span>

<span class="sd">        Note that in binary classification, recall of the positive class</span>
<span class="sd">        is also known as &quot;sensitivity&quot;; recall of the negative class is</span>
<span class="sd">        &quot;specificity&quot;.</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    precision_recall_fscore_support, confusion_matrix,</span>
<span class="sd">    multilabel_confusion_matrix</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import classification_report</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 1, 2, 2, 2]</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 1]</span>
<span class="sd">    &gt;&gt;&gt; target_names = [&#39;class 0&#39;, &#39;class 1&#39;, &#39;class 2&#39;]</span>
<span class="sd">    &gt;&gt;&gt; print(classification_report(y_true, y_pred, target_names=target_names))</span>
<span class="sd">                  precision    recall  f1-score   support</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">         class 0       0.50      1.00      0.67         1</span>
<span class="sd">         class 1       0.00      0.00      0.00         1</span>
<span class="sd">         class 2       1.00      0.67      0.80         3</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">        accuracy                           0.60         5</span>
<span class="sd">       macro avg       0.50      0.56      0.49         5</span>
<span class="sd">    weighted avg       0.70      0.60      0.61         5</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [1, 1, 0]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [1, 1, 1]</span>
<span class="sd">    &gt;&gt;&gt; print(classification_report(y_true, y_pred, labels=[1, 2, 3]))</span>
<span class="sd">                  precision    recall  f1-score   support</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">               1       1.00      0.67      0.80         3</span>
<span class="sd">               2       0.00      0.00      0.00         0</span>
<span class="sd">               3       0.00      0.00      0.00         0</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">       micro avg       1.00      0.67      0.80         3</span>
<span class="sd">       macro avg       0.33      0.22      0.27         3</span>
<span class="sd">    weighted avg       1.00      0.67      0.80         3</span>
<span class="sd">    &lt;BLANKLINE&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="n">labels_given</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="n">labels_given</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="c1"># labelled micro average</span>
    <span class="n">micro_is_accuracy</span> <span class="o">=</span> <span class="p">((</span><span class="n">y_type</span> <span class="o">==</span> <span class="s1">&#39;multiclass&#39;</span> <span class="ow">or</span> <span class="n">y_type</span> <span class="o">==</span> <span class="s1">&#39;binary&#39;</span><span class="p">)</span> <span class="ow">and</span>
                         <span class="p">(</span><span class="ow">not</span> <span class="n">labels_given</span> <span class="ow">or</span>
                          <span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))))</span>

    <span class="k">if</span> <span class="n">target_names</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">labels_given</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;labels size, </span><span class="si">{0}</span><span class="s2">, does not match size of target_names, </span><span class="si">{1}</span><span class="s2">&quot;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Number of classes, </span><span class="si">{0}</span><span class="s2">, does not match size of &quot;</span>
                <span class="s2">&quot;target_names, </span><span class="si">{1}</span><span class="s2">. Try specifying the labels &quot;</span>
                <span class="s2">&quot;parameter&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">target_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>

    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="s2">&quot;recall&quot;</span><span class="p">,</span> <span class="s2">&quot;f1-score&quot;</span><span class="p">,</span> <span class="s2">&quot;support&quot;</span><span class="p">]</span>
    <span class="c1"># compute per-class results without averaging</span>
    <span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
                                                  <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                                  <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                  <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">target_names</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f1</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="n">average_options</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="s1">&#39;samples&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">average_options</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="s1">&#39;weighted&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_dict</span><span class="p">:</span>
        <span class="n">report_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">label</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">scores</span> <span class="ow">in</span> <span class="n">report_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">report_dict</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">headers</span><span class="p">,</span>
                                          <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">longest_last_line_heading</span> <span class="o">=</span> <span class="s1">&#39;weighted avg&#39;</span>
        <span class="n">name_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cn</span><span class="p">)</span> <span class="k">for</span> <span class="n">cn</span> <span class="ow">in</span> <span class="n">target_names</span><span class="p">)</span>
        <span class="n">width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">name_width</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">longest_last_line_heading</span><span class="p">),</span> <span class="n">digits</span><span class="p">)</span>
        <span class="n">head_fmt</span> <span class="o">=</span> <span class="s1">&#39;{:&gt;</span><span class="si">{width}</span><span class="s1">s} &#39;</span> <span class="o">+</span> <span class="s1">&#39; </span><span class="si">{:&gt;9}</span><span class="s1">&#39;</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">headers</span><span class="p">)</span>
        <span class="n">report</span> <span class="o">=</span> <span class="n">head_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">headers</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">)</span>
        <span class="n">report</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span>
        <span class="n">row_fmt</span> <span class="o">=</span> <span class="s1">&#39;{:&gt;</span><span class="si">{width}</span><span class="s1">s} &#39;</span> <span class="o">+</span> <span class="s1">&#39; {:&gt;9.</span><span class="si">{digits}</span><span class="s1">f}&#39;</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="s1">&#39; </span><span class="si">{:&gt;9}</span><span class="se">\n</span><span class="s1">&#39;</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
            <span class="n">report</span> <span class="o">+=</span> <span class="n">row_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">row</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="n">digits</span><span class="p">)</span>
        <span class="n">report</span> <span class="o">+=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

    <span class="c1"># compute all applicable averages</span>
    <span class="k">for</span> <span class="n">average</span> <span class="ow">in</span> <span class="n">average_options</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">average</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">micro_is_accuracy</span><span class="p">:</span>
            <span class="n">line_heading</span> <span class="o">=</span> <span class="s1">&#39;accuracy&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">line_heading</span> <span class="o">=</span> <span class="n">average</span> <span class="o">+</span> <span class="s1">&#39; avg&#39;</span>

        <span class="c1"># compute averages with specified averaging method</span>
        <span class="n">avg_p</span><span class="p">,</span> <span class="n">avg_r</span><span class="p">,</span> <span class="n">avg_f1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span>
            <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="p">[</span><span class="n">avg_p</span><span class="p">,</span> <span class="n">avg_r</span><span class="p">,</span> <span class="n">avg_f1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span>

        <span class="k">if</span> <span class="n">output_dict</span><span class="p">:</span>
            <span class="n">report_dict</span><span class="p">[</span><span class="n">line_heading</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">headers</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">avg</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">line_heading</span> <span class="o">==</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span>
                <span class="n">row_fmt_accuracy</span> <span class="o">=</span> <span class="s1">&#39;{:&gt;</span><span class="si">{width}</span><span class="s1">s} &#39;</span> <span class="o">+</span> \
                        <span class="s1">&#39; {:&gt;9.</span><span class="si">{digits}</span><span class="s1">}&#39;</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="s1">&#39; {:&gt;9.</span><span class="si">{digits}</span><span class="s1">f}&#39;</span> <span class="o">+</span> \
                        <span class="s1">&#39; </span><span class="si">{:&gt;9}</span><span class="se">\n</span><span class="s1">&#39;</span>
                <span class="n">report</span> <span class="o">+=</span> <span class="n">row_fmt_accuracy</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">line_heading</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                                                  <span class="o">*</span><span class="n">avg</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
                                                  <span class="n">digits</span><span class="o">=</span><span class="n">digits</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">report</span> <span class="o">+=</span> <span class="n">row_fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">line_heading</span><span class="p">,</span> <span class="o">*</span><span class="n">avg</span><span class="p">,</span>
                                         <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span> <span class="n">digits</span><span class="o">=</span><span class="n">digits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;accuracy&#39;</span> <span class="ow">in</span> <span class="n">report_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">report_dict</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">report_dict</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">][</span><span class="s1">&#39;precision&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">report_dict</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">report</span>


<span class="k">def</span> <span class="nf">hamming_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the average Hamming loss.</span>

<span class="sd">    The Hamming loss is the fraction of labels that are incorrectly predicted.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;hamming_loss&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Ground truth (correct) labels.</span>

<span class="sd">    y_pred : 1d array-like, or label indicator array / sparse matrix</span>
<span class="sd">        Predicted labels, as returned by a classifier.</span>

<span class="sd">    labels : array, shape = [n_labels], optional (default=&#39;deprecated&#39;)</span>
<span class="sd">        Integer array of labels. If not provided, labels will be inferred</span>
<span class="sd">        from y_true and y_pred.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">        .. deprecated:: 0.21</span>
<span class="sd">           This parameter ``labels`` is deprecated in version 0.21 and will</span>
<span class="sd">           be removed in version 0.23. Hamming loss uses ``y_true.shape[1]``</span>
<span class="sd">           for the number of labels when y_true is binary label indicators,</span>
<span class="sd">           so it is unnecessary for the user to specify.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float or int,</span>
<span class="sd">        Return the average Hamming loss between element of ``y_true`` and</span>
<span class="sd">        ``y_pred``.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    accuracy_score, jaccard_score, zero_one_loss</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    In multiclass classification, the Hamming loss corresponds to the Hamming</span>
<span class="sd">    distance between ``y_true`` and ``y_pred`` which is equivalent to the</span>
<span class="sd">    subset ``zero_one_loss`` function, when `normalize` parameter is set to</span>
<span class="sd">    True.</span>

<span class="sd">    In multilabel classification, the Hamming loss is different from the</span>
<span class="sd">    subset zero-one loss. The zero-one loss considers the entire set of labels</span>
<span class="sd">    for a given sample incorrect if it does not entirely match the true set of</span>
<span class="sd">    labels. Hamming loss is more forgiving in that it penalizes only the</span>
<span class="sd">    individual labels.</span>

<span class="sd">    The Hamming loss is upperbounded by the subset zero-one loss, when</span>
<span class="sd">    `normalize` parameter is set to True. It is always between 0 and 1,</span>
<span class="sd">    lower being better.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:</span>
<span class="sd">           An Overview. International Journal of Data Warehousing &amp; Mining,</span>
<span class="sd">           3(3), 1-13, July-September 2007.</span>

<span class="sd">    .. [2] `Wikipedia entry on the Hamming distance</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Hamming_distance&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import hamming_loss</span>
<span class="sd">    &gt;&gt;&gt; y_pred = [1, 2, 3, 4]</span>
<span class="sd">    &gt;&gt;&gt; y_true = [2, 2, 3, 4]</span>
<span class="sd">    &gt;&gt;&gt; hamming_loss(y_true, y_pred)</span>
<span class="sd">    0.25</span>

<span class="sd">    In the multilabel case with binary label indicators:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))</span>
<span class="sd">    0.75</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">y_type</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">_check_targets</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The labels parameter is unused. It was&quot;</span>
                      <span class="s2">&quot; deprecated in version 0.21 and&quot;</span>
                      <span class="s2">&quot; will be removed in version 0.23&quot;</span><span class="p">,</span>
                      <span class="ne">DeprecationWarning</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">weight_average</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weight_average</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">y_type</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;multilabel&#39;</span><span class="p">):</span>
        <span class="n">n_differences</span> <span class="o">=</span> <span class="n">count_nonzero</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">,</span>
                                      <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">n_differences</span> <span class="o">/</span>
                <span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight_average</span><span class="p">))</span>

    <span class="k">elif</span> <span class="n">y_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_weighted_sum</span><span class="p">(</span><span class="n">y_true</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{0}</span><span class="s2"> is not supported&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_type</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">log_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Log loss, aka logistic loss or cross-entropy loss.</span>

<span class="sd">    This is the loss function used in (multinomial) logistic regression</span>
<span class="sd">    and extensions of it such as neural networks, defined as the negative</span>
<span class="sd">    log-likelihood of the true labels given a probabilistic classifier&#39;s</span>
<span class="sd">    predictions. The log loss is only defined for two or more labels.</span>
<span class="sd">    For a single sample with true label yt in {0,1} and</span>
<span class="sd">    estimated probability yp that yt = 1, the log loss is</span>

<span class="sd">        -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;log_loss&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array-like or label indicator matrix</span>
<span class="sd">        Ground truth (correct) labels for n_samples samples.</span>

<span class="sd">    y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)</span>
<span class="sd">        Predicted probabilities, as returned by a classifier&#39;s</span>
<span class="sd">        predict_proba method. If ``y_pred.shape = (n_samples,)``</span>
<span class="sd">        the probabilities provided are assumed to be that of the</span>
<span class="sd">        positive class. The labels in ``y_pred`` are assumed to be</span>
<span class="sd">        ordered alphabetically, as done by</span>
<span class="sd">        :class:`preprocessing.LabelBinarizer`.</span>

<span class="sd">    eps : float</span>
<span class="sd">        Log loss is undefined for p=0 or p=1, so probabilities are</span>
<span class="sd">        clipped to max(eps, min(1 - eps, p)).</span>

<span class="sd">    normalize : bool, optional (default=True)</span>
<span class="sd">        If true, return the mean loss per sample.</span>
<span class="sd">        Otherwise, return the sum of the per-sample losses.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    labels : array-like, optional (default=None)</span>
<span class="sd">        If not provided, labels will be inferred from y_true. If ``labels``</span>
<span class="sd">        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are</span>
<span class="sd">        assumed to be binary and are inferred from ``y_true``.</span>
<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import log_loss</span>
<span class="sd">    &gt;&gt;&gt; log_loss([&quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;],  # doctest: +ELLIPSIS</span>
<span class="sd">    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])</span>
<span class="sd">    0.21616...</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,</span>
<span class="sd">    p. 209.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The logarithm used is the natural logarithm (base-e).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">lb</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;y_true contains only one label (</span><span class="si">{0}</span><span class="s1">). Please &#39;</span>
                             <span class="s1">&#39;provide the true labels explicitly through the &#39;</span>
                             <span class="s1">&#39;labels argument.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The labels array needs to contain at least two &#39;</span>
                             <span class="s1">&#39;labels for log_loss, &#39;</span>
                             <span class="s1">&#39;got </span><span class="si">{0}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

    <span class="n">transformed_labels</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">transformed_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">transformed_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">transformed_labels</span><span class="p">,</span>
                                       <span class="n">transformed_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Clipping</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>

    <span class="c1"># If y_pred is of single dimension, assume y_true to be binary</span>
    <span class="c1"># and then check.</span>
    <span class="k">if</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Check if dimensions are consistent.</span>
    <span class="n">transformed_labels</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">transformed_labels</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">!=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y_true and y_pred contain different number of &quot;</span>
                             <span class="s2">&quot;classes </span><span class="si">{0}</span><span class="s2">, </span><span class="si">{1}</span><span class="s2">. Please provide the true &quot;</span>
                             <span class="s2">&quot;labels explicitly through the labels argument. &quot;</span>
                             <span class="s2">&quot;Classes found in &quot;</span>
                             <span class="s2">&quot;y_true: </span><span class="si">{2}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">transformed_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                  <span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                  <span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The number of classes in labels is different &#39;</span>
                             <span class="s1">&#39;from that in y_pred. Classes found in &#39;</span>
                             <span class="s1">&#39;labels: </span><span class="si">{0}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lb</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>

    <span class="c1"># Renormalize</span>
    <span class="n">y_pred</span> <span class="o">/=</span> <span class="n">y_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">transformed_labels</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_weighted_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">normalize</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hinge_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_decision</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Average hinge loss (non-regularized)</span>

<span class="sd">    In binary class case, assuming labels in y_true are encoded with +1 and -1,</span>
<span class="sd">    when a prediction mistake is made, ``margin = y_true * pred_decision`` is</span>
<span class="sd">    always negative (since the signs disagree), implying ``1 - margin`` is</span>
<span class="sd">    always greater than 1.  The cumulated hinge loss is therefore an upper</span>
<span class="sd">    bound of the number of mistakes made by the classifier.</span>

<span class="sd">    In multiclass case, the function expects that either all the labels are</span>
<span class="sd">    included in y_true or an optional labels argument is provided which</span>
<span class="sd">    contains all the labels. The multilabel margin is calculated according</span>
<span class="sd">    to Crammer-Singer&#39;s method. As in the binary case, the cumulated hinge loss</span>
<span class="sd">    is an upper bound of the number of mistakes made by the classifier.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;hinge_loss&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array, shape = [n_samples]</span>
<span class="sd">        True target, consisting of integers of two values. The positive label</span>
<span class="sd">        must be greater than the negative label.</span>

<span class="sd">    pred_decision : array, shape = [n_samples] or [n_samples, n_classes]</span>
<span class="sd">        Predicted decisions, as output by decision_function (floats).</span>

<span class="sd">    labels : array, optional, default None</span>
<span class="sd">        Contains all the labels for the problem. Used in multiclass hinge loss.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss : float</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry on the Hinge loss</span>
<span class="sd">           &lt;https://en.wikipedia.org/wiki/Hinge_loss&gt;`_</span>

<span class="sd">    .. [2] Koby Crammer, Yoram Singer. On the Algorithmic</span>
<span class="sd">           Implementation of Multiclass Kernel-based Vector</span>
<span class="sd">           Machines. Journal of Machine Learning Research 2,</span>
<span class="sd">           (2001), 265-292</span>

<span class="sd">    .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models</span>
<span class="sd">           by Robert C. Moore, John DeNero.</span>
<span class="sd">           &lt;http://www.ttic.edu/sigml/symposium2011/papers/</span>
<span class="sd">           Moore+DeNero_Regularization.pdf&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import svm</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import hinge_loss</span>
<span class="sd">    &gt;&gt;&gt; X = [[0], [1]]</span>
<span class="sd">    &gt;&gt;&gt; y = [-1, 1]</span>
<span class="sd">    &gt;&gt;&gt; est = svm.LinearSVC(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; est.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="sd">         intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="sd">         multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=0, tol=0.0001,</span>
<span class="sd">         verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; pred_decision = est.decision_function([[-2], [3], [0.5]])</span>
<span class="sd">    &gt;&gt;&gt; pred_decision  # doctest: +ELLIPSIS</span>
<span class="sd">    array([-2.18...,  2.36...,  0.09...])</span>
<span class="sd">    &gt;&gt;&gt; hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.30...</span>

<span class="sd">    In the multiclass case:</span>

<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[0], [1], [2], [3]])</span>
<span class="sd">    &gt;&gt;&gt; Y = np.array([0, 1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; labels = np.array([0, 1, 2, 3])</span>
<span class="sd">    &gt;&gt;&gt; est = svm.LinearSVC()</span>
<span class="sd">    &gt;&gt;&gt; est.fit(X, Y)  # doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="sd">         intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="sd">         multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,</span>
<span class="sd">         verbose=0)</span>
<span class="sd">    &gt;&gt;&gt; pred_decision = est.decision_function([[-1], [2], [3]])</span>
<span class="sd">    &gt;&gt;&gt; y_true = [0, 2, 3]</span>
<span class="sd">    &gt;&gt;&gt; hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS</span>
<span class="sd">    0.56...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_decision</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>
    <span class="n">pred_decision</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">pred_decision</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_true_unique</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_true_unique</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pred_decision</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span>
                <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">y_true_unique</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pred_decision</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please include all labels in y_true &quot;</span>
                             <span class="s2">&quot;or pass labels as third argument&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">y_true_unique</span>
        <span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
        <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pred_decision</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y_true</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">margin</span> <span class="o">=</span> <span class="n">pred_decision</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span>
        <span class="n">margin</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pred_decision</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_true</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                         <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Handles binary class case</span>
        <span class="c1"># this code assumes that positive and negative labels</span>
        <span class="c1"># are encoded as +1 and -1 respectively</span>
        <span class="n">pred_decision</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">pred_decision</span><span class="p">)</span>
        <span class="n">pred_decision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">pred_decision</span><span class="p">)</span>

        <span class="n">lbin</span> <span class="o">=</span> <span class="n">LabelBinarizer</span><span class="p">(</span><span class="n">neg_label</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">lbin</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y_true</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">margin</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">*</span> <span class="n">pred_decision</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;pred_decision should be an array of floats.&quot;</span><span class="p">)</span>

    <span class="n">losses</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">margin</span>
    <span class="c1"># The hinge_loss doesn&#39;t penalize good enough predictions.</span>
    <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the Brier score.</span>
<span class="sd">    The smaller the Brier score, the better, hence the naming with &quot;loss&quot;.</span>
<span class="sd">    Across all items in a set N predictions, the Brier score measures the</span>
<span class="sd">    mean squared difference between (1) the predicted probability assigned</span>
<span class="sd">    to the possible outcomes for item i, and (2) the actual outcome.</span>
<span class="sd">    Therefore, the lower the Brier score is for a set of predictions, the</span>
<span class="sd">    better the predictions are calibrated. Note that the Brier score always</span>
<span class="sd">    takes on a value between zero and one, since this is the largest</span>
<span class="sd">    possible difference between a predicted probability (which must be</span>
<span class="sd">    between zero and one) and the actual outcome (which can take on values</span>
<span class="sd">    of only 0 and 1). The Brier loss is composed of refinement loss and</span>
<span class="sd">    calibration loss.</span>
<span class="sd">    The Brier score is appropriate for binary and categorical outcomes that</span>
<span class="sd">    can be structured as true or false, but is inappropriate for ordinal</span>
<span class="sd">    variables which can take on three or more values (this is because the</span>
<span class="sd">    Brier score assumes that all possible outcomes are equivalently</span>
<span class="sd">    &quot;distant&quot; from one another). Which label is considered to be the positive</span>
<span class="sd">    label is controlled via the parameter pos_label, which defaults to 1.</span>
<span class="sd">    Read more in the :ref:`User Guide &lt;calibration&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array, shape (n_samples,)</span>
<span class="sd">        True targets.</span>

<span class="sd">    y_prob : array, shape (n_samples,)</span>
<span class="sd">        Probabilities of the positive class.</span>

<span class="sd">    sample_weight : array-like of shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>

<span class="sd">    pos_label : int or str, default=None</span>
<span class="sd">        Label of the positive class.</span>
<span class="sd">        Defaults to the greater label unless y_true is all 0 or all -1</span>
<span class="sd">        in which case pos_label defaults to 1.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        Brier score</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.metrics import brier_score_loss</span>
<span class="sd">    &gt;&gt;&gt; y_true = np.array([0, 1, 1, 0])</span>
<span class="sd">    &gt;&gt;&gt; y_true_categorical = np.array([&quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;])</span>
<span class="sd">    &gt;&gt;&gt; y_prob = np.array([0.1, 0.9, 0.8, 0.3])</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.037...</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.037...</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true_categorical, y_prob, \</span>
<span class="sd">                         pos_label=&quot;ham&quot;)  # doctest: +ELLIPSIS</span>
<span class="sd">    0.037...</span>
<span class="sd">    &gt;&gt;&gt; brier_score_loss(y_true, np.array(y_prob) &gt; 0.5)</span>
<span class="sd">    0.0</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] `Wikipedia entry for the Brier score.</span>
<span class="sd">            &lt;https://en.wikipedia.org/wiki/Brier_score&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_prob</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y_prob</span><span class="p">)</span>
    <span class="n">assert_all_finite</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">assert_all_finite</span><span class="p">(</span><span class="n">y_prob</span><span class="p">)</span>
    <span class="n">check_consistent_length</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Only binary classification is supported. &quot;</span>
                         <span class="s2">&quot;Labels in y_true: </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_prob</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y_prob contains values greater than 1.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y_prob</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y_prob contains values less than 0.&quot;</span><span class="p">)</span>

    <span class="c1"># if pos_label=None, when y_true is in {-1, 1} or {0, 1},</span>
    <span class="c1"># pos_labe is set to 1 (consistent with precision_recall_curve/roc_curve),</span>
    <span class="c1"># otherwise pos_label is set to the greater label</span>
    <span class="c1"># (different from precision_recall_curve/roc_curve,</span>
    <span class="c1"># the purpose is to keep backward compatibility).</span>
    <span class="k">if</span> <span class="n">pos_label</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])):</span>
            <span class="n">pos_label</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pos_label</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_true</span> <span class="o">==</span> <span class="n">pos_label</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_prob</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>