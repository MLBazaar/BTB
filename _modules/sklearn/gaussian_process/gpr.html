

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.gaussian_process.gpr &mdash; BTB 0.3.0.dev0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../../../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#install">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-using-pip">Install using Pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-for-development">Install for Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#quickstart">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#tuners">Tuners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../readme.html#selectors">Selectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#what-s-next">Whatâ€™s next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#citing-btb">Citing BTB</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/btb.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.selection.html">btb.selection package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#module-btb.selection">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.tuning.html">btb.tuning package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#module-btb.tuning">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#module-btb">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id1">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id2">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id3">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id4">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id5">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id6">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id7">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id8">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id9">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id10">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id11">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id12">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id13">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id14">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id15">0.1.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sklearn.gaussian_process.gpr</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sklearn.gaussian_process.gpr</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Gaussian processes regression. &quot;&quot;&quot;</span>

<span class="c1"># Authors: Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">itemgetter</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="k">import</span> <span class="n">cholesky</span><span class="p">,</span> <span class="n">cho_solve</span><span class="p">,</span> <span class="n">solve_triangular</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="k">import</span> <span class="n">fmin_l_bfgs_b</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="k">import</span> <span class="n">MultiOutputMixin</span>
<span class="kn">from</span> <span class="nn">.kernels</span> <span class="k">import</span> <span class="n">RBF</span><span class="p">,</span> <span class="n">ConstantKernel</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="k">import</span> <span class="n">check_random_state</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="k">import</span> <span class="n">check_X_y</span><span class="p">,</span> <span class="n">check_array</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="k">import</span> <span class="n">ConvergenceWarning</span>


<span class="k">class</span> <span class="nc">GaussianProcessRegressor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span>
                               <span class="n">MultiOutputMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gaussian process regression (GPR).</span>

<span class="sd">    The implementation is based on Algorithm 2.1 of Gaussian Processes</span>
<span class="sd">    for Machine Learning (GPML) by Rasmussen and Williams.</span>

<span class="sd">    In addition to standard scikit-learn estimator API,</span>
<span class="sd">    GaussianProcessRegressor:</span>

<span class="sd">       * allows prediction without prior fitting (based on the GP prior)</span>
<span class="sd">       * provides an additional method sample_y(X), which evaluates samples</span>
<span class="sd">         drawn from the GPR (prior or posterior) at given inputs</span>
<span class="sd">       * exposes a method log_marginal_likelihood(theta), which can be used</span>
<span class="sd">         externally for other ways of selecting hyperparameters, e.g., via</span>
<span class="sd">         Markov chain Monte Carlo.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;gaussian_process&gt;`.</span>

<span class="sd">    .. versionadded:: 0.18</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    kernel : kernel object</span>
<span class="sd">        The kernel specifying the covariance function of the GP. If None is</span>
<span class="sd">        passed, the kernel &quot;1.0 * RBF(1.0)&quot; is used as default. Note that</span>
<span class="sd">        the kernel&#39;s hyperparameters are optimized during fitting.</span>

<span class="sd">    alpha : float or array-like, optional (default: 1e-10)</span>
<span class="sd">        Value added to the diagonal of the kernel matrix during fitting.</span>
<span class="sd">        Larger values correspond to increased noise level in the observations.</span>
<span class="sd">        This can also prevent a potential numerical issue during fitting, by</span>
<span class="sd">        ensuring that the calculated values form a positive definite matrix.</span>
<span class="sd">        If an array is passed, it must have the same number of entries as the</span>
<span class="sd">        data used for fitting and is used as datapoint-dependent noise level.</span>
<span class="sd">        Note that this is equivalent to adding a WhiteKernel with c=alpha.</span>
<span class="sd">        Allowing to specify the noise level directly as a parameter is mainly</span>
<span class="sd">        for convenience and for consistency with Ridge.</span>

<span class="sd">    optimizer : string or callable, optional (default: &quot;fmin_l_bfgs_b&quot;)</span>
<span class="sd">        Can either be one of the internally supported optimizers for optimizing</span>
<span class="sd">        the kernel&#39;s parameters, specified by a string, or an externally</span>
<span class="sd">        defined optimizer passed as a callable. If a callable is passed, it</span>
<span class="sd">        must have the signature::</span>

<span class="sd">            def optimizer(obj_func, initial_theta, bounds):</span>
<span class="sd">                # * &#39;obj_func&#39; is the objective function to be minimized, which</span>
<span class="sd">                #   takes the hyperparameters theta as parameter and an</span>
<span class="sd">                #   optional flag eval_gradient, which determines if the</span>
<span class="sd">                #   gradient is returned additionally to the function value</span>
<span class="sd">                # * &#39;initial_theta&#39;: the initial value for theta, which can be</span>
<span class="sd">                #   used by local optimizers</span>
<span class="sd">                # * &#39;bounds&#39;: the bounds on the values of theta</span>
<span class="sd">                ....</span>
<span class="sd">                # Returned are the best found hyperparameters theta and</span>
<span class="sd">                # the corresponding value of the target function.</span>
<span class="sd">                return theta_opt, func_min</span>

<span class="sd">        Per default, the &#39;fmin_l_bfgs_b&#39; algorithm from scipy.optimize</span>
<span class="sd">        is used. If None is passed, the kernel&#39;s parameters are kept fixed.</span>
<span class="sd">        Available internal optimizers are::</span>

<span class="sd">            &#39;fmin_l_bfgs_b&#39;</span>

<span class="sd">    n_restarts_optimizer : int, optional (default: 0)</span>
<span class="sd">        The number of restarts of the optimizer for finding the kernel&#39;s</span>
<span class="sd">        parameters which maximize the log-marginal likelihood. The first run</span>
<span class="sd">        of the optimizer is performed from the kernel&#39;s initial parameters,</span>
<span class="sd">        the remaining ones (if any) from thetas sampled log-uniform randomly</span>
<span class="sd">        from the space of allowed theta-values. If greater than 0, all bounds</span>
<span class="sd">        must be finite. Note that n_restarts_optimizer == 0 implies that one</span>
<span class="sd">        run is performed.</span>

<span class="sd">    normalize_y : boolean, optional (default: False)</span>
<span class="sd">        Whether the target values y are normalized, i.e., the mean of the</span>
<span class="sd">        observed target values become zero. This parameter should be set to</span>
<span class="sd">        True if the target values&#39; mean is expected to differ considerable from</span>
<span class="sd">        zero. When enabled, the normalization effectively modifies the GP&#39;s</span>
<span class="sd">        prior based on the data, which contradicts the likelihood principle;</span>
<span class="sd">        normalization is thus disabled per default.</span>

<span class="sd">    copy_X_train : bool, optional (default: True)</span>
<span class="sd">        If True, a persistent copy of the training data is stored in the</span>
<span class="sd">        object. Otherwise, just a reference to the training data is stored,</span>
<span class="sd">        which might cause predictions to change if the data is modified</span>
<span class="sd">        externally.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default: None)</span>
<span class="sd">        The generator used to initialize the centers. If int, random_state is</span>
<span class="sd">        the seed used by the random number generator; If RandomState instance,</span>
<span class="sd">        random_state is the random number generator; If None, the random number</span>
<span class="sd">        generator is the RandomState instance used by `np.random`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    X_train_ : array-like, shape = (n_samples, n_features)</span>
<span class="sd">        Feature values in training data (also required for prediction)</span>

<span class="sd">    y_train_ : array-like, shape = (n_samples, [n_output_dims])</span>
<span class="sd">        Target values in training data (also required for prediction)</span>

<span class="sd">    kernel_ : kernel object</span>
<span class="sd">        The kernel used for prediction. The structure of the kernel is the</span>
<span class="sd">        same as the one passed as parameter but with optimized hyperparameters</span>

<span class="sd">    L_ : array-like, shape = (n_samples, n_samples)</span>
<span class="sd">        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``</span>

<span class="sd">    alpha_ : array-like, shape = (n_samples,)</span>
<span class="sd">        Dual coefficients of training data points in kernel space</span>

<span class="sd">    log_marginal_likelihood_value_ : float</span>
<span class="sd">        The log-marginal-likelihood of ``self.kernel_.theta``</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_friedman2</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.gaussian_process import GaussianProcessRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_friedman2(n_samples=500, noise=0, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; kernel = DotProduct() + WhiteKernel()</span>
<span class="sd">    &gt;&gt;&gt; gpr = GaussianProcessRegressor(kernel=kernel,</span>
<span class="sd">    ...         random_state=0).fit(X, y)</span>
<span class="sd">    &gt;&gt;&gt; gpr.score(X, y) # doctest: +ELLIPSIS</span>
<span class="sd">    0.3680...</span>
<span class="sd">    &gt;&gt;&gt; gpr.predict(X[:2,:], return_std=True) # doctest: +ELLIPSIS</span>
<span class="sd">    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;fmin_l_bfgs_b&quot;</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">normalize_y</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">copy_X_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span> <span class="o">=</span> <span class="n">n_restarts_optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize_y</span> <span class="o">=</span> <span class="n">normalize_y</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">copy_X_train</span> <span class="o">=</span> <span class="n">copy_X_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit Gaussian process regression model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">            Training data</span>

<span class="sd">        y : array-like, shape = (n_samples, [n_output_dims])</span>
<span class="sd">            Target values</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># Use an RBF kernel as default</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span> <span class="o">=</span> <span class="n">C</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">constant_value_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span> \
                <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">y_numeric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Normalize target value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_y</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># demean y</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">iterable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> \
           <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be a scalar or an array&quot;</span>
                                 <span class="s2">&quot; with same number of entries as y.(</span><span class="si">%d</span><span class="s2"> != </span><span class="si">%d</span><span class="s2">)&quot;</span>
                                 <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_X_train</span> <span class="k">else</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">copy_X_train</span> <span class="k">else</span> <span class="n">y</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">n_dims</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Choose hyperparameters based on maximizing the log-marginal</span>
            <span class="c1"># likelihood (potentially starting from several initial values)</span>
            <span class="k">def</span> <span class="nf">obj_func</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>
                    <span class="n">lml</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">(</span>
                        <span class="n">theta</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="k">return</span> <span class="o">-</span><span class="n">lml</span><span class="p">,</span> <span class="o">-</span><span class="n">grad</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

            <span class="c1"># First optimize starting from theta specified in kernel</span>
            <span class="n">optima</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">_constrained_optimization</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">theta</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">bounds</span><span class="p">))]</span>

            <span class="c1"># Additional runs are performed from log-uniform chosen initial</span>
            <span class="c1"># theta</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Multiple optimizer restarts (n_restarts_optimizer&gt;0) &quot;</span>
                        <span class="s2">&quot;requires that all bounds are finite.&quot;</span><span class="p">)</span>
                <span class="n">bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">bounds</span>
                <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_restarts_optimizer</span><span class="p">):</span>
                    <span class="n">theta_initial</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">bounds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bounds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
                    <span class="n">optima</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_constrained_optimization</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">theta_initial</span><span class="p">,</span>
                                                       <span class="n">bounds</span><span class="p">))</span>
            <span class="c1"># Select result from run with minimal (negative) log-marginal</span>
            <span class="c1"># likelihood</span>
            <span class="n">lml_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">optima</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">optima</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">lml_values</span><span class="p">)][</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood_value_</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">lml_values</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood_value_</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span>

        <span class="c1"># Precompute quantities required for predictions which are independent</span>
        <span class="c1"># of actual query points</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
        <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">L_</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Line 2</span>
            <span class="c1"># self.L_ changed, self._K_inv needs to be recomputed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_K_inv</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
            <span class="n">exc</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;The kernel, </span><span class="si">%s</span><span class="s2">, is not returning a &quot;</span>
                        <span class="s2">&quot;positive definite matrix. Try gradually &quot;</span>
                        <span class="s2">&quot;increasing the &#39;alpha&#39; parameter of your &quot;</span>
                        <span class="s2">&quot;GaussianProcessRegressor estimator.&quot;</span>
                        <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">,)</span> <span class="o">+</span> <span class="n">exc</span><span class="o">.</span><span class="n">args</span>
            <span class="k">raise</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span> <span class="o">=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_</span><span class="p">)</span>  <span class="c1"># Line 3</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict using the Gaussian process regression model</span>

<span class="sd">        We can also predict based on an unfitted model by using the GP prior.</span>
<span class="sd">        In addition to the mean of the predictive distribution, also its</span>
<span class="sd">        standard deviation (return_std=True) or covariance (return_cov=True).</span>
<span class="sd">        Note that at most one of the two can be requested.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">            Query points where the GP is evaluated</span>

<span class="sd">        return_std : bool, default: False</span>
<span class="sd">            If True, the standard-deviation of the predictive distribution at</span>
<span class="sd">            the query points is returned along with the mean.</span>

<span class="sd">        return_cov : bool, default: False</span>
<span class="sd">            If True, the covariance of the joint predictive distribution at</span>
<span class="sd">            the query points is returned along with the mean</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_mean : array, shape = (n_samples, [n_output_dims])</span>
<span class="sd">            Mean of predictive distribution a query points</span>

<span class="sd">        y_std : array, shape = (n_samples,), optional</span>
<span class="sd">            Standard deviation of predictive distribution at query points.</span>
<span class="sd">            Only returned when return_std is True.</span>

<span class="sd">        y_cov : array, shape = (n_samples, n_samples), optional</span>
<span class="sd">            Covariance of joint predictive distribution a query points.</span>
<span class="sd">            Only returned when return_cov is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">return_std</span> <span class="ow">and</span> <span class="n">return_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Not returning standard deviation of predictions when &quot;</span>
                <span class="s2">&quot;returning full covariance.&quot;</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;X_train_&quot;</span><span class="p">):</span>  <span class="c1"># Unfitted;predict based on GP prior</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">kernel</span> <span class="o">=</span> <span class="p">(</span><span class="n">C</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">constant_value_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span> <span class="o">*</span>
                          <span class="n">RBF</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
                <span class="n">y_cov</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
            <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="n">y_var</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># Predict based on GP posterior</span>
            <span class="n">K_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>  <span class="c1"># Line 4 (y_mean = f_star)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span> <span class="o">+</span> <span class="n">y_mean</span>  <span class="c1"># undo normal.</span>
            <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># Line 5</span>
                <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>  <span class="c1"># Line 6</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
            <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="c1"># cache result of K_inv computation</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_K_inv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># compute inverse K_inv of K based on its Cholesky</span>
                    <span class="c1"># decomposition L and its inverse L_inv</span>
                    <span class="n">L_inv</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                             <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_K_inv</span> <span class="o">=</span> <span class="n">L_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_inv</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

                <span class="c1"># Compute variance of predictive distribution</span>
                <span class="n">y_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">y_var</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ij,ij-&gt;i&quot;</span><span class="p">,</span>
                                   <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_trans</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_K_inv</span><span class="p">),</span> <span class="n">K_trans</span><span class="p">)</span>

                <span class="c1"># Check if any of the variances is negative because of</span>
                <span class="c1"># numerical issues. If yes: set the variance to 0.</span>
                <span class="n">y_var_negative</span> <span class="o">=</span> <span class="n">y_var</span> <span class="o">&lt;</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y_var_negative</span><span class="p">):</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Predicted variances smaller than 0. &quot;</span>
                                  <span class="s2">&quot;Setting those variances to 0.&quot;</span><span class="p">)</span>
                    <span class="n">y_var</span><span class="p">[</span><span class="n">y_var_negative</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span>

    <span class="k">def</span> <span class="nf">sample_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Draw samples from Gaussian process and evaluate at X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape = (n_samples_X, n_features)</span>
<span class="sd">            Query points where the GP samples are evaluated</span>

<span class="sd">        n_samples : int, default: 1</span>
<span class="sd">            The number of samples drawn from the Gaussian process</span>

<span class="sd">        random_state : int, RandomState instance or None, optional (default=0)</span>
<span class="sd">            If int, random_state is the seed used by the random number</span>
<span class="sd">            generator; If RandomState instance, random_state is the</span>
<span class="sd">            random number generator; If None, the random number</span>
<span class="sd">            generator is the RandomState instance used by `np.random`.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y_samples : array, shape = (n_samples_X, [n_output_dims], n_samples)</span>
<span class="sd">            Values of n_samples samples drawn from Gaussian process and</span>
<span class="sd">            evaluated at query points.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

        <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y_mean</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_samples</span> <span class="o">=</span> \
                <span class="p">[</span><span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">y_mean</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">y_cov</span><span class="p">,</span>
                                         <span class="n">n_samples</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_mean</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
            <span class="n">y_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">y_samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_samples</span>

    <span class="k">def</span> <span class="nf">log_marginal_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Returns log-marginal likelihood of theta for training data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        theta : array-like, shape = (n_kernel_params,) or None</span>
<span class="sd">            Kernel hyperparameters for which the log-marginal likelihood is</span>
<span class="sd">            evaluated. If None, the precomputed log_marginal_likelihood</span>
<span class="sd">            of ``self.kernel_.theta`` is returned.</span>

<span class="sd">        eval_gradient : bool, default: False</span>
<span class="sd">            If True, the gradient of the log-marginal likelihood with respect</span>
<span class="sd">            to the kernel hyperparameters at position theta is returned</span>
<span class="sd">            additionally. If True, theta must not be None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        log_likelihood : float</span>
<span class="sd">            Log-marginal likelihood of theta for training data.</span>

<span class="sd">        log_likelihood_gradient : array, shape = (n_kernel_params,), optional</span>
<span class="sd">            Gradient of the log-marginal likelihood with respect to the kernel</span>
<span class="sd">            hyperparameters at position theta.</span>
<span class="sd">            Only returned when eval_gradient is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">theta</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Gradient can only be evaluated for theta!=None&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_marginal_likelihood_value_</span>

        <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">clone_with_theta</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>
            <span class="n">K</span><span class="p">,</span> <span class="n">K_gradient</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">,</span> <span class="n">eval_gradient</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">K</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>

        <span class="n">K</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">diag_indices_from</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Line 2</span>
        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta</span><span class="p">))</span> \
                <span class="k">if</span> <span class="n">eval_gradient</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="c1"># Support multi-dimensional output of self.y_train_</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_</span>
        <span class="k">if</span> <span class="n">y_train</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

        <span class="n">alpha</span> <span class="o">=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="n">L</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># Line 3</span>

        <span class="c1"># Compute log-likelihood (compare line 7)</span>
        <span class="n">log_likelihood_dims</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ik,ik-&gt;k&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">log_likelihood_dims</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">L</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">log_likelihood_dims</span> <span class="o">-=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">log_likelihood_dims</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># sum over dimensions</span>

        <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>  <span class="c1"># compare Equation 5.9 from GPML</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ik,jk-&gt;ijk&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>  <span class="c1"># k: output-dimension</span>
            <span class="n">tmp</span> <span class="o">-=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="n">L</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[:,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
            <span class="c1"># Compute &quot;0.5 * trace(tmp.dot(K_gradient))&quot; without</span>
            <span class="c1"># constructing the full matrix tmp.dot(K_gradient) since only</span>
            <span class="c1"># its diagonal is required</span>
            <span class="n">log_likelihood_gradient_dims</span> <span class="o">=</span> \
                <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ijl,ijk-&gt;kl&quot;</span><span class="p">,</span> <span class="n">tmp</span><span class="p">,</span> <span class="n">K_gradient</span><span class="p">)</span>
            <span class="n">log_likelihood_gradient</span> <span class="o">=</span> <span class="n">log_likelihood_gradient_dims</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">eval_gradient</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_likelihood</span><span class="p">,</span> <span class="n">log_likelihood_gradient</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">log_likelihood</span>

    <span class="k">def</span> <span class="nf">_constrained_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obj_func</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">bounds</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s2">&quot;fmin_l_bfgs_b&quot;</span><span class="p">:</span>
            <span class="n">theta_opt</span><span class="p">,</span> <span class="n">func_min</span><span class="p">,</span> <span class="n">convergence_dict</span> <span class="o">=</span> \
                <span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">convergence_dict</span><span class="p">[</span><span class="s2">&quot;warnflag&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;fmin_l_bfgs_b terminated abnormally with the &quot;</span>
                              <span class="s2">&quot; state: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">convergence_dict</span><span class="p">,</span>
                              <span class="n">ConvergenceWarning</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">):</span>
            <span class="n">theta_opt</span><span class="p">,</span> <span class="n">func_min</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">obj_func</span><span class="p">,</span> <span class="n">initial_theta</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown optimizer </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">theta_opt</span><span class="p">,</span> <span class="n">func_min</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>